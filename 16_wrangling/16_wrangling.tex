\documentclass[aspectratio=169,12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}

\input{../LaTeX/header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Wrangling messy data files}
\author{\href{https://kbroman.org}{Karl Broman}}
\institute{Biostatistics \& Medical Informatics, UW{\textendash}Madison}
\date{\href{https://kbroman.org}{\tt \scriptsize \color{foreground} kbroman.org}
\\[-4pt]
\href{https://github.com/kbroman}{\tt \scriptsize \color{foreground} github.com/kbroman}
\\[-4pt]
\href{https://twitter.com/kwbroman}{\tt \scriptsize \color{foreground} @kwbroman}
\\[-4pt]
{\scriptsize Course web: \href{https://kbroman.org/AdvData}{\tt kbroman.org/AdvData}}
}

\begin{document}

{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

\note{
  In this lecture, we'll look at the problem of wrangling messy data
  files: A bit of data diagnostics, but mostly how to reorganize data
  files.
}
} }


\begin{frame}[c]{}

\centering
\Large

{
\color{title}

``In what form would you like the data?''
}

\bigskip \bigskip

\onslide<2->{
{\hilit

    ``In its present form!''
}
}

\bigskip \bigskip \bigskip \bigskip
\onslide<3>{
\lolit
\large
\qquad \qquad \qquad   ...so we'll have some messy files to deal with.
}

\note{
  When collaborators ask me how I would like them to send me data, I
  always say: in its present form.

  I cannot emphasize enough: If any transformation needs to be
  done, or if anything needs to be fixed, it is the data scientist who
  is in the best position to do the work, reproducibly and without
  introducing further errors.

  But that means I spend a lot of time mucking about with some pretty
  messy files.  In the lecture today, I want to impart some tips on
  things I've learned, doing so.
}

\end{frame}


\begin{frame}{Challenges}

  \bigskip

  {\large \vhilit Consistency}

  \bigskip

  \bi
\item file names
\item file organization
\item subject IDs
\item variable names
\item categorical data
  \ei

    \note{
      Essentially all of the challenges come from inconsistencies: in
      file names, the arrangement of data within files, the subject
      identifiers, the variable names, and the categories in
      categorical data.

      Code re-organizing data is the worst code.
    }

\end{frame}



\begin{frame}[c]{Example file}

  \figw{Figs/pheno_data.png}{1.0}

  \note{
    Here's an example file. Lots of work was done to prettify things,
    which means multiple header rows and a good amount of work to
    identify and pull out the essential columns.
  }
\end{frame}


\begin{frame}[c]{Another example}

  \figw{Figs/necropsy_data.png}{1.0}

  \note{
    Here's a second worksheet from that file. Again, the header row
    has information on multiple lines that need to be merged. With the
    merged cells, it's hard to predict where they end up in a CSV file.

    The format of individual identifiers is constantly changing.
  }
\end{frame}


\begin{frame}{Weird rounding}

    \figw{Figs/weird_rounding.png}{1.0}

\note{
  Part of that file shows some weird rounding patterns. The font isn't
  even consistent. This suggests to me that there has been some
  copy-pasting of data, and that there may be some other set of files
  that is the primary source for the data.
}
\end{frame}


\begin{frame}{Inconsistent IDs}

  \includegraphics[width=0.8\textwidth]{Figs/attiedo_wave2_sheet1.png}

  \only<2>{
    \vspace{-0.5\textheight}
    \hspace{0.1\textwidth}
    \includegraphics[width=0.8\textwidth]{Figs/attiedo_wave5_sheet1.png}
  }

\note{
  The format of the IDs is different between these files. Also in one
  of the files, there are missing dates that will need to be grabbed
  from some separate file.
}
\end{frame}





\begin{frame}[c]{Inconsistent layout}

  \begin{columns}
    \column{0.5\textwidth}

    \figw{Figs/attiedo_wave2_gtt.png}{1.0}

    \column{0.5\textwidth}

    \figh{Figs/attiedo_wave3_gtt.png}{0.8}
  \end{columns}


\note{
  Another example of inconsistent layout. And messy, in both cases.
  You need to fill in the repeated values like mouse ID, and the
  column names are missing in the file on the right.
}
\end{frame}


\begin{frame}{All kinds of inconsistencies}

  \includegraphics[height=0.6\textheight]{Figs/attiedo_wave2_sacwts.png}

  \only<2->{
    \vspace{-0.5\textheight}
    \hspace{0.1\textwidth}
  \includegraphics[height=0.6\textheight]{Figs/attiedo_wave3_sacwts.png}
  }

  \only<3->{
    \vspace{-0.5\textheight}
    \hspace{0.2\textwidth}
  \includegraphics[height=0.6\textheight]{Figs/attiedo_wave4_sacwts.png}
  }

  \only<4>{
    \vspace{-0.5\textheight}
    \hspace{0.3\textwidth}
  \includegraphics[height=0.6\textheight]{Figs/attiedo_wave5_sacwts.png}
  }

\note{
  The layouts, IDs, and included information are all inconsistent here.
}
\end{frame}




\begin{frame}[c]{Multiple rectangles}

\figw{Figs/attiedo_multicolumn.png}{1.0}

\note{
  Here's an example where they have a group of columns with one set of
  data, a few blank columns, then a group of columns with another set
  of data.
}

\end{frame}







\begin{frame}{Stuff moving around}

  \includegraphics[width=0.8\textwidth]{Figs/attiedo_do118_exvivo.png}

  \only<2>{
    \vspace{-0.65\textheight}
    \hspace{0.1\textwidth}
    \includegraphics[width=0.8\textwidth]{Figs/attiedo_do121_exvivo.png}
  }


  \note{
    This has a super-complicated layout, has 500 worksheets with one
    mouse each, and the order of things aren't entirely consistent.
  }
\end{frame}



\begin{frame}{Being self-sufficient}

  \bbi
\item {\hilit C}
\item {\hilit P}erl (or python or ruby\only<2>{ or R})
\item {\hilit R}
  \ei

    \note{
      I've long said that for a data scientist to be self-sufficient,
      they should be savvy with multiple programming languages.

      R for data analysis. C for when you need high-performance, and
      for like 15 years I used Perl for manipulating data files plus
      shell scripting.

      But I'd now say use Python or Ruby instead of Perl; probably
      Python. And really, I think we can now do everything we want to
      do in R. It's a perfectly sufficient general programming language.
    }

\end{frame}




\begin{frame}{Key techniques}

  \bbi
\item stepping through a file
\item regular expressions
  \bi
  \item search and replace patterns
  \ei
\item parsing individual lines in a file
\item matching vectors
\item construct meta data
\item system calls
  \ei

\note{
  Here are some of the key techniques that I use to wrangle messy data
  files. I'll go through many of these in more detail.
}


\end{frame}








\begin{frame}[fragile,c]{Stepping through a file in R}

\begin{lstlisting}
  filecon <- open("huge_data.txt", "r")
  while(TRUE) {

    line <- readLines(filecon, n=1)
    if( grepl("^\\[Data\\]", line) )  break

  }

  data <- readLines(filecon)
  close(filecon)
\end{lstlisting}


\note{
  I often use {\tt readLines()} to slurp up an entire file as a vector
  of character strings.

  For really big files, I might want to read one or a few lines at a
  time instead, and throw out all but the stuff that matters. You can
  do this in R!

  This particular example will skip over a header that ends with
  [Data] and then read in everything after that.
}
\end{frame}







\begin{frame}[fragile]{Regular expressions}

  \only<1|handout 0>{\figh{Figs/xkcd_regular_expressions.png}{0.8}

    \vspace{4mm}

    \hfill \href{https://xkcd.com/208}{\tt xkcd.com/208} }
  \only<2|handout 0>{\figh{Figs/regex_cover.jpg}{0.9} }

  \only<3>{
    \bigskip

  {\hilit {\tt grep()}, {\tt grepl()}, {\tt sub()}, {\tt gsub()}}

  \bigskip

  \bi
\item {\tt \textasciicircum} and {\tt \$} match the beginning and end of a line
\item {\tt [034]} for any one of several things; {\tt [0-9]} for a range
\item {\tt [{\textasciicircum}034]} for something {\hilit other} than this set of things
\item {\tt {\textbackslash}s} for white space
\item {\tt .} match any one character
\item {\tt +} match the last bit 1 or more times
\item {\tt *} match the last bit 0 or more times
\item parentheses to group bits for use with {\tt +} and {\tt *}
\item when substituting, can use {\tt {\textbackslash}1}, {\tt {\textbackslash}2}, ... in place of
  matched groups
\item In R, most backslashes need to be made double-backslashes.
\ei
}

\note{
  Regular expression are {\vhilit} hugely useful for the data
  wrangling work.
}
\end{frame}







\begin{frame}{Parsing strings}

\note{
  A lot of {\tt strsplit()} and then messing about with lists.

  Consider {\tt stringr} package.
}
\end{frame}






\begin{frame}{Matching vectors}

\note{
  Mostly {\tt match()}. Need to check for missing values in the result.

  Often use this to reorder the rows or columns in one data set to
  match the rows or columns in another data set.
}
\end{frame}






\begin{frame}[c]{Construct meta data}


\figh{Figs/attiedo_metadata_example.png}{0.9}


\note{
  This is an example meta data file that I set up because each batch
  of subjects in a project had the data organized completely
  differently.

  I identified the variables of interest and chose a fixed set of
  names.

  Then for each batch, I worked out which file it was in, the name of
  the column to look for, and then I also needed a column ``offset''
  because to find week 4 measurements for variable X, you might look
  for the column that is two to the right from the one labeled Y.
}
\end{frame}





\begin{frame}{R challenges}

  \bbi
\item {\tt stringsAsFactors}
\item {\tt check.names} in {\tt read.csv()}
\item dealing with factors
  \bi
\item levels
\item converting to/from strings
  \ei
  \ei

\note{
 Mention the forcats package.
}
\end{frame}


\begin{frame}{Further tips}

\bbi
\item Avoid using numeric indices
  \bi
\item refer to data by variable name and individual ID
\item this will be more {\hilit robust}
  \ei
\item {\tt glue} package vs {\tt paste}, {\tt paste0}
\item {\tt stopifnot()} to assert things that should be true
\item {\tt cbind} and {\tt rbind}, but padding with missing values
\item Sometimes converting excel -> csv loses precision
  \ei

  \note{
  }
\end{frame}




\begin{frame}{Verify everything}

\bbi
\item subject IDs unique?
\item derived columns
\item (other stuff from eda lecture)
\ei

  \note{
  }
\end{frame}




\begin{frame}{Reproducible reports}

\bbi
\item You want all of this work to be reproducible
\item Consider combining the data reorganization with the data
  cleaning
  \bi
    \item a lot of double-checking is happening when reorganizing
  \ei
\item Or clean each file one at a time
  \bi
    \item do the detailed diagnostics and cross-checks with data that are
      in a more convenient form
  \ei
\item Include diagnostic plots
  \bi
    \item Plot stuff vs time
    \item Scatterplots of different variables
    \item Look at missing data pattern
  \ei
\item Explain your thought process and observations
  \ei

  \note{
  }
\end{frame}








\end{document}
