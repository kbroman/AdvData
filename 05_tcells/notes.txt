- Likelihood

- EM algorithm
  - interative algorithm, useful when there is missing data that if
    observed would make things easy
  - starting values of parameters
  - E-step: expected value of missing data given current parameter values
  - M-step: MLEs replacing missing data with their expected values
  - Advantages:
    - often super easy to code
    - usually quite stable
    - log likelihood is non-decreasing

- This case

Pr(k = s | y, parameters) = Pr(k=s) Pr(y|k=s) / sum_s Pr(k=s) Pr(y|k=s)

E(k | y, parameters) = sum_s s Pr(k=s) Pr(y|k=s) / sum_s Pr(k=s) Pr(y|k=s)

- Oops it's harder than I said
  - expected log likelihood
  - usually function of the sufficient statistics

- Here, we need not just E(k|y) but also E(k^2|y)


- X'X = ((n sum(k)) (sum(k) sum(k^2)))
  X'y = (sum(y) sum(k y))
  SE?

  RSS = y'y - (y'X) (X'X)^-1 (X'y)

- Difficulties
  - starting values
  - multiple modes
  (would be nice to show an example)

- Box-Cox transformation

- SEs via SEM algorithm
  - point to book _EM algorithm and its extensions_


Lessons:
- start with an understanding of the problem
- think about a model for the process
- EM algorithm is really useful
- Use log likelihood as a diagnostic when implementing an EM algorithm


Impact:
- I'm pretty sure that the vaccine they were working on didn't work well
- R package "npem" but I never put it on CRAN, and no one has ever
  asked me about it
- Our paper has like 9 citations; no one has every really used the method
