\documentclass[aspectratio=169,12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}

\input{../LaTeX/header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{The EM algorithm}
\subtitle{Analysis of a T cell frequency assay}
\author{\href{https://kbroman.org}{Karl Broman}}
\institute{Biostatistics \& Medical Informatics, UW{\textendash}Madison}
\date{\href{https://kbroman.org}{\tt \scriptsize \color{foreground} kbroman.org}
\\[-4pt]
\href{https://github.com/kbroman}{\tt \scriptsize \color{foreground} github.com/kbroman}
\\[-4pt]
\href{https://twitter.com/kwbroman}{\tt \scriptsize \color{foreground} @kwbroman}
\\[-4pt]
{\scriptsize Course web: \href{https://kbroman.org/AdvData}{\tt kbroman.org/AdvData}}
}


\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \note{}

} }


\begin{frame}[c]{}

  \begin{columns}[c]
    \column{0.3\textwidth}
    \fontsize{10pt}{11}\selectfont
      \bbi
      \item[\hilit Goal:] Estimate the frequency of T-cells in a blood
        sample that respond to two test antigens.

      \item[\hilit Real goal:] Determine whether a vaccine causes an
        increase in the frequency of responding T-cells.
      \ei

      \vspace{10mm}

      \hspace*{-0.2\textwidth} {\fontsize{6pt}{6}\selectfont Broman K, Speed T, Tigges M (1996)
        J Immunol Meth 198:119-132
        \href{https://doi.org/b54v33}{\tt doi.org/b54v33}}


    \column{0.5\textwidth}
        \figw{Figs/immunology.png}{1.0}

  \end{columns}


\end{frame}



\begin{frame}[c]{The assay}

  \begin{columns}[c]
    \column{0.5\textwidth}
    \fontsize{10pt}{11}\selectfont

    \bbi
  \item Combine:
    \bi
  \item diluted blood cells + growth medium
  \item antigen
  \item $^{\text{3}}$H-thymidine
    \ei

    \item Replicating cells take up $^{\text{3}}$H-thymidine.

    \item Extract the DNA and measure its radioactivity
      \ei

    \column{0.5\textwidth}
        \figw{Figs/assay.png}{1.0}

  \end{columns}

\end{frame}


\begin{frame}{Usual approaches}

  \bbi

\item Use 3 wells with antigen and 3 wells without antigen,\\
  and take the ratio of the averages

  \item Limiting dilution assay
    \bi
  \item Several dilutions of cells
  \item Many wells at each dilution
    \ei
    \ei

\end{frame}


\begin{frame}[c]{Our assay}

  \begin{columns}[c]
    \column{0.5\textwidth}
    Study a single plate or pair of plates at a single dilution.

    \column{0.5\textwidth}
    \figw{Figs/microtiter_plate.png}{1.0}


  \end{columns}


\end{frame}

\begin{frame}[c]{Data}
  \figh{Figs/lda713_data.png}{1.0}
\end{frame}


\begin{frame}[c]{Traditional analysis}
  \bbi
  \item Split wells into +/-- using a cutoff (e.g., mean + 3 SD of
    ``cells alone'' wells)
    \bi
  \item[] positive = one or more responding cells
  \item[] negative = no responding cells
    \ei

  \item Imagine that the number of responding cells in a well is
    Poisson($\lambda_i$) for group $i$

    \vspace{4mm}

  \qquad Pr(no responding cells) = $e^{-\lambda_i}$

    \vspace{4mm}

  \qquad $\hat{\lambda}_i = -\log\left(\frac{\text{\# negative wells}}{\text{\# wells}}\right)$

    \ei

\end{frame}

\begin{frame}[c]{Analysis}
  \figh{Figs/simple_method.png}{0.9}
\end{frame}



\begin{frame}{Problems}
  \bbi
\item Hard to choose cutoff
\item Potential loss of information
  \ei
\end{frame}


\begin{frame}{Response vs no. cells}
\figw{Figs/mean_response.pdf}{1.0}
\end{frame}


\begin{frame}[c]{Model}

  \vspace{3mm}

  $k_{ij}$ = Number of responding cells (unobserved)

  $y_{ij}$ = square-root of response

  \vspace{10mm}

  Assume {\hilit $k_{ij}$ $\sim \text{Poisson}(\lambda_i)$}

  \vspace{2mm}

  \hspace*{15mm} {\hilit $y_{ij}$ $|$ $k_{ij}$ $\sim \text{Normal}(a +
    b k_{ij}, \sigma)$}

  \vspace{2mm}

  \hspace*{15mm} {\hilit ($k_{ij}$, $y_{ij}$) mutually independent}

  \vspace{5mm}

  \figw{Figs/model_distribution.pdf}{1.0}

\end{frame}




\begin{frame}[c]{log Likelihood}


  \begin{eqnarray*}
    l(\boldsymbol{\lambda}, a, b, \sigma)
       & = & \sum_{i,j} \log \text{Pr}(y_{ij} | \lambda_i, a, b, \sigma) \\
    & = & \sum_{i,j} \log \left[ \sum_k
      \text{Pr}(k | \lambda_i)
      \text{Pr}(y_{ij} | k, a, b, \sigma)
      \right] \\
    & = & \sum_{i,j} \log \left[ \sum_k
      \left(\frac{e^{-\lambda_i} \lambda_i^k}{ k! }\right) \, \phi\left( \frac{y_{ij} - a -
        bk}{\sigma} \right)
      \right]
  \end{eqnarray*}

\end{frame}



\begin{frame}{EM algorithm}

\vspace{-3mm}

  \bbi
  \item Iterative algorithm useful when there is missing data that if
    observed would make things easy
  \item Dempster et al. (1977) JRSS-B 39:1-22
    \href{https://doi.org/gfxzrv}{\tt doi.org/gfxzrv}
  \item Start with some initial estimates
  \item {\hilit E-step}: expected value of missing data given current estimates
  \item {\hilit M-step}: MLEs replacing missing data with their expected values
  \item {\hilit Advantages}
    \bi
  \item often easy to code
  \item usually super stable
  \item log likelihood is non-decreasing
    \ei
    \ei

\end{frame}



\begin{frame}{Normal/Poisson model}

  {\hilit E-step}:

\vspace{-3mm}

  \begin{eqnarray*}
      \text{Pr}(k = s | y, \lambda, a, b, \sigma) & = & \frac{\text{Pr}(k = s | \lambda)
      \text{Pr}(y | k = s, a, b, \sigma)}{\sum_s \text{Pr}(k = s | \lambda)
        \text{Pr}(y | k = s, a, b, \sigma)} \\
      & = & \frac{\left(\frac{e^{-\lambda} \lambda^s}{s!}\right) \phi\left(\frac{y-a-bs}{\sigma}\right)}{
\sum_s \left(\frac{e^{-\lambda} \lambda^s}{s!}\right)
\phi\left(\frac{y-a-bs}{\sigma}\right)} \\[18pt]
      \text{E}(k | y, \lambda, a, b, \sigma)
      & = & \frac{\sum_s s \left(\frac{e^{-\lambda}
          \lambda^s}{s!}\right)
        \phi\left(\frac{y-a-bs}{\sigma}\right)}{\sum_s
        \left(\frac{e^{-\lambda} \lambda^s}{s!}\right)
        \phi\left(\frac{y-a-bs}{\sigma}\right)}
  \end{eqnarray*}


  \vspace{5mm}

  {\hilit M-step}: Regress y on $\text{E}(k|y)$


\end{frame}




\begin{frame}{Oops, that didn't work}
\figw{Figs/em_loglik_wrong.pdf}{1.0}
\end{frame}




\begin{frame}{EM algorithm, more formally}

  \bbi

  \item Calculate expected complete-data log likelihood, given
    observed data and observed parameters, and then maximize that.

    $$ l^{(s)}(\theta) = \text{E}\{\log f(y, k | \theta) | y,
    \hat{\theta}^{(s)}\} $$

  \item In practice, it's usually a linear combination of the
    sufficient statistics, so you focus on those.

  \item Here, we need not just $\sum k$ and $\sum k y$, but also $\sum k^2$.

    \ei

\end{frame}

\begin{frame}{EM algorithm, again}

{\hilit E step}: we also need

\vspace{-4mm}

\begin{eqnarray*}
      \text{E}(k^2 | y, \lambda, a, b, \sigma)
      & = & \frac{\sum_s s^2 \left(\frac{e^{-\lambda}
          \lambda^s}{s!}\right)
        \phi\left(\frac{y-a-bs}{\sigma}\right)}{\sum_s
        \left(\frac{e^{-\lambda} \lambda^s}{s!}\right)
        \phi\left(\frac{y-a-bs}{\sigma}\right)}
\end{eqnarray*}

\vspace{5mm}

{\hilit M step}: we want $\hat{\beta} = (X'X)^{-1}(X'y)$

\begin{eqnarray*}
\text{where }  (X'X) & \text{ is like } &
  \begin{pmatrix} n & \sum k \\ \sum k & \sum k^2 \end{pmatrix}
  \\[18pt]
\text{ and }  (X'y) & \text{ is like } &
  \begin{pmatrix}\sum y \\ \sum k y \end{pmatrix}
\end{eqnarray*}



\end{frame}



\begin{frame}{Ah, that's better}
\figw{Figs/em_loglik_right.pdf}{1.0}
\end{frame}


\begin{frame}{Difficulties}

\bbi
\item Starting values
\item Multiple modes
\ei

\end{frame}




\begin{frame}[c]{Multiple modes}
\figw{Figs/multiple_modes.pdf}{1.0}
\end{frame}




\begin{frame}[c]{Multiple modes}

\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{rrrrrrrrrr}
  \hline
 & $\lambda\_0$ & $\lambda\_D$ & $\lambda\_B$ & $\lambda\_T$ & $a$ &
  $b$ & $\sigma$ & log lik & no. hits \\
  \hline
1 & 0.32 & 3.03 & 2.82 & 4.37 & 16.73 & 10.34 & 3.52 & --289.73 & 331 \\
  2 & 1.18 & 5.40 & 4.95 & 7.49 & 12.16 & 6.69 & 2.15 & --289.80 & 26 \\
  3 & 0.17 & 2.10 & 1.95 & 3.07 & 17.44 & 14.56 & 4.18 & --290.50 & 415 \\
  4 & 0.51 & 3.89 & 3.56 & 5.58 & 15.72 & 8.35 & 3.58 & --290.70 & 180 \\
  5 & 0.73 & 4.62 & 4.25 & 6.58 & 14.58 & 7.27 & 3.43 & --291.08 & 30 \\
  6 & 1.64 & 6.79 & 6.29 & 9.35 & 10.81 & 5.51 & 1.89 & --291.40 & 7 \\
  7 & 1.57 & 6.22 & 5.80 & 8.61 & 10.60 & 6.02 & 2.13 & --291.59 & 10 \\
  8 & 2.59 & 7.76 & 7.25 & 10.34 & 5.75 & 5.47 & 1.88 & --292.27 & 1 \\
   \hline
\end{tabular}

\end{frame}


\begin{frame}[c]{Estimate vs. starting point}
\figw{Figs/starting_pts.pdf}{1.0}
\end{frame}




\begin{frame}{Principles}

  \bbi
\item Start with an understanding of the problem and data
\item Think about a model for the data-generating process
  \ei
\end{frame}



\begin{frame}{Lessons}

  \bbi
\item The EM algorithm is really useful
\item Use the log likelihood as a diagnostic when implementing an EM
  algorithm
  \ei
\end{frame}


\begin{frame}{Software development time}

  \bbi
\item Formulating the problem
\item Writing the code
\item Debugging the code
\item Executing the code
  \ei

\end{frame}


\begin{frame}{Impact}

  \bbi
\item I'm pretty sure that the vaccine they were working on didn't
  work well.
\item R package \href{https://github.com/kbroman/npem}{\tt npem}, but
  I never put it on \href{https://cran.r-project.org}{CRAN}, and no
  one has ever asked me about it.
\item Our paper has like 9 citations: no one has ever really used the
  method.
  \ei

\end{frame}


\begin{frame}{Further things}

  \bbi

\item Standard errors should always be required.
  \bi
\item But usually painful to obtain
\item We used the SEM algorithm of Meng and Rubin (1991)
  \href{https://doi.org/10.1080/01621459.1991.10475130}{\tt
    doi.org/10.1080/01621459.1991.10475130}
  \ei

\item Could more formally investigate the appropriate transformation
  \bi
\item See Box and Cox (1964) \href{https://doi.org/10.1111/j.2517-6161.1964.tb00553.x}{
  \tt doi.org/10.1111/j.2517-6161.1964.tb00553.x}
\item Box-Cox transformation is $g(y) = (y^c-1)/c$ for $c \ne 0$ and
  $= \log y$ for $c=0$
\item Key issue is change-of-variables in the density; as a result you
  add $\sum_{ij} (c-1) \log y_{ij}$ to the log likelihood
    \ei
    \ei
\end{frame}


\end{document}
